\documentclass{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[german]{babel}
\usepackage{amssymb}
\usepackage{fancyhdr}
\usepackage[framemethod=TikZ]{mdframed}
\usepackage{amsthm}
\usepackage[german,vlined,longend]{algorithm2e}
\usepackage{amsmath}
\usepackage[T1]{fontenc}
\usepackage{mathabx}

\title{DataMining CheatSheet}
\author{Julian Schubert}

% Definition
\mdfdefinestyle{theoremstyle}{
    linecolor=blue!20,
    linewidth=2pt,
    frametitlerule=true,
    frametitlebackgroundcolor=gray!20,
    innertopmargin=\topskip
}
\mdtheorem[style=theoremstyle]{definition}{Definition}

% Eigenschaft
\mdfdefinestyle{eigenschaftstyle}{
    linecolor=red!50,
    linewidth=2pt,
    frametitlerule=true,
    frametitlebackgroundcolor=gray!20,
    innertopmargin=\topskip
}
\mdtheorem[style=eigenschaftstyle]{eigenschaft}{Eigenschaft}


% Kopf- / Fußzeile
\makeatletter
\let\runauthor\@author
\let\runtitle\@title
\pagestyle{fancy}
\fancyhf{}
\rhead{\runtitle}
\lhead{\runauthor}
\cfoot{\thepage}

\newcommand{\subsubsubsection}[1]{\paragraph{#1}\mbox{}\\}
\setcounter{secnumdepth}{4}
\setcounter{tocdepth}{4}

\begin{document}

\maketitle

\section{Gütemaße}
\subsection{Davies-Bouldin Index (DB)}
    \begin{tabular}{| c | c |}
        \hline
        Güte innerhalb des Clusters $C_i$ 
            & $S_i \sqrt[q]{\frac{1}{|C_i|} \sum_{x \in C_i}\text{dist}(x, \mu_i)^q}$ \\
        \hline
        Güte Trennung $C_i$ und $C_j$
            & $M_{i,j} = $dist$(\mu_i, \mu_j)$ \\
        \hline
        $R_{i,j}$ für $i \neq j$
            & $R_{i, j} = \frac{S_i + S_j}{M_{i, j}}$ \\
        \hline
        Davis-Bouldin Index
            & $DB = \frac{1}{k} \sum_{i = 1}^k D_i$ mit $D_i = \max_{i \neq j} R_{i,j}$ \\
        \hline
    \end{tabular}
\section{Distanzfunktionen}
\subsection{Distanzfunktionen für Cluster}
    \begin{tabular}{| c | c |}
        \hline
        Single Link 
            & dist - sl(X, Y) = $\min_{x \in X, y \in Y}$ dist(x, y) \\
        \hline
        Complete Link
            & dist - cl(X, Y) = $\max_{x \in X, y \in Y}$ dist(x, y) \\
        \hline
        Average Link
            & dist - al(X, Y) = $\frac{1}{|X| \cdot |Y|} \cdot \sum_{x \in X, y\in Y}$ dist(x, y) \\
        \hline
    \end{tabular}
\section{Dichtebasiertes Clustern}
\begin{itemize}
    \item \textbf{Kernobjekt}: Mehr als MinPts in $\epsilon$-Umgebung
    \item \textbf{direkt dichte-erreichbar}: $p \in N_{\epsilon}(q)$ und 
    $q$ ist Kernobjekt
    \item \textbf{dichte-erreichbar}: Kette von dichte-erreichbaren
    Objekten zwischen q und p
    \item \textbf{dichte-verbunden} Beide von einem dritten Objekt
    dichte-erreichbar
\end{itemize}
\section{DBSCAN}
Beschreibung in Worten:
\begin{enumerate}
    \item Wählt zufälligen noch nicht klassifizierten Punkt
    \item Führt ExpandiereCluster für diesen Punkt aus
    \item \textbf{ExpandiereCluster}:
    \begin{itemize}
        \item Punkt ist Noise -> FALSE zurück geben
        \item Sonst: Füge alle dichte-erreichbaren Punkte vom gegebenen
        Punkt zum Cluster hinzu
    \end{itemize}
\end{enumerate}
\section{OPTICS}
\begin{itemize}
    \item \textbf{Kerndistanz} eines Objekts $o$ und bzg. $\epsilon$ und MinPts:
    \begin{itemize}
        \item undefiniert, wenn $|N_{\epsilon}(o)| < MinPts$
        \item Distanz$_{\text{MinPts}}$ sonst
    \end{itemize}
    \item \textbf{Distanz$_k$(o)}: Distanz zum k-nächsten
    Nachbarn von o
    \item \textbf{Erreichbarkeitsdistanz} eines Objekts $p$
    relativ zu einem Objekt o
    \begin{itemize}
        \item undefiniert, wenn $|N_{\epsilon}(o)| < MinPts$
        \item max\{Kerndistanz(o), dist(o, p)\} sonst
    \end{itemize}
\end{itemize}
Beschreibung in Worten: 
\begin{enumerate}
    \item Über alle Punkte iterieren
    \item Wenn Punkte im Umkreis vom aktuellen Punkt liegen Distanzen updaten
    \item Alle Nachbarn vom Punkt abarbeiten
    \item Sortiert in die Liste einfügen
\end{enumerate}
\section{Assoziationsregeln}
\begin{itemize}
    \item \textbf{Support}: $supp_D(X) = \frac{|\{ T \in D |X \subseteq T\}|}{|D|}$
    \item \textbf{Frequency:} $supp_X(D) \cdot |D|$
    \item \textbf{Confidence:} $conf_D(X \rightarrow Y) = \frac{supp_D(X \cup Y)}{supp_D(X)}$
\end{itemize}
\section{Auswahl von Assoziationsregeln}
\subsection{Added Value}
\begin{center}
    $
        \frac{sup (A \land B)}{sup (A)} - sup(B) = conf(A \rightarrow B) - sup(B)
    $
\end{center}
Um wie viel steigt die Wahrscheinlichkeit von B, wenn die Bedingung A
Hinzugefügt wird?
\subsection{Kriterien für Interessantheitsmaße}
\begin{enumerate}
    \item Conciseness: \\
    Kürzere Regeln sind besser (weniger Items)
    \item Generality: \\
    Generelle Regeln sind besser (mehr Fälle abgedeckt)
    \item Reliability: \\
    Hohe confidence / accuracy ist besser
    \item Diversity: \\
    Regeln sollten untereinander unähnlich sein
    \item Novelty: \\
    Vorher unbekannt, nicht aus anderen Regeln ableitbar
    \item Surprisingness / Unexpectedness: \\
    Gute Regeln widersprechen Vorwissen / Erwartungen
    \item Applicability: \\ 
    Kann praktisch (in der Anwendung) umgesetzt werden
\end{enumerate}
\subsection{Monotonie}
\begin{itemize}
    \item \textbf{Monotonie}: If a set S violates C, its supersets
    \textbf{might not} violate C, while its subsets \textbf{must}
    violate C
    \item \textbf{Anti-Monotonie}: If a set S violates C, its supersets
    \textbf{must} violate C, while its subsets \textbf{might not}
    violate C
\end{itemize}
\section{Naive Bayes}
Entscheidungsregel des naiven Bayes Klassifikators:
\begin{center}
    argmax$_{c_j \in C} P(c_j) \cdot \prod_{i = 1}^{d} P(o_i | c_j)$ 
\end{center}
\section{Hierarchische Assoziationsregeln}
\begin{itemize}
    \item \textbf{Definition} Hierarchische Assoziationsregel: \\
    $X \Rightarrow Y$, mit $X \subseteq I, Y \subseteq I, X \cap Y = \emptyset$ \\
    Kein Item in Y ist sVorfahre eines Items in X (bezüglich H)
    \item \textbf{Support s} einer hierarchischen Assoziationsregel
    $X \Rightarrow Y$ in D: \\
    Support der Menge $X \cup Y$
    \item \textbf{Konfidenz c} einer hierarchischen Assoziationsregel
    $X \Rightarrow Y$ in D: \\
    Prozentsatz der Transaktionen, die auch die Menge Y unterstützen,
    in der Teilmenge aller Transaktionen, welche die Menge X unterstützen
\end{itemize}
\section{Gütemaße für Klassifikation}
\textbf{K}: Klassifikator, \textbf{TR} Trainingsmenge, \textbf{TE} 
Testmenge
\begin{itemize}
    \item \textbf{Klassifikationsgenauigkeit} $G_{TE}$: \\
    Alles was aus dem Testset richig klassifiziert wurde
    \item \textbf{Tatsächlicher Klassifikationsfehler} $F_{TE}$: \\
    Alles was aus dem Testset falsch klassifiziert wurde
    \item \textbf{Beobachteter Klassifikationsfehler} $F_{TR}$: \\
    Alles was aus dem Trainingsset falsch klassifiziert wurde
\end{itemize}
\section{Precision und Recall}
\textbf{Precision}: $\frac{\text{True positive}}{\text{True Positive} + 
\text{false Positive}}$ \\
Wenn die klasse vorhergesagt wird, wie sicher ist die Vorhersage \\
\\
\textbf{Recall}: $\frac{\text{True positive}}{\text{True Positive} + 
\text{False Negative}}$ \\
Wie oft wird die Klasse c wieder gefunden
\section{Gütemaße für Splits}
\subsection{Informationsgewinn}
\textbf{Entropie}: \\
$entropie(T) = - \sum_{i = 1}^{k} p_i \log(p_i)$ \\
\\
\textbf{Informationsgewinn}: \\
$informationsgewinn(T, A) = entropie(T) -
\sum_{i = 1}^{m} \frac{|T_i|}{|T|} \cdot entropie(T_i)$
\subsection{Gini-Index}
\begin{center}
    $gini(T) = 1 - \sum_{j = 1}^k p_j^2$
\end{center}
Kleiner Gini-Index $\Leftrightarrow$ geringe Unreinheit \\
Großer Gini-Index $\Leftrightarrow$ hohe Unreinheit \\
Gini-Index des Attributs A in Bezug auf T ist definiert als
\begin{center}
    $gini_A(T) = \sum_{i = 1}^m \frac{|T_i|}{|T|} \cdot gini(T_i)$
\end{center}
\section{Delta-Rule}
\begin{center}
    $w = w + \eta \cdot x \cdot (t - o)$
\end{center}
Vereinfacht: \\
\begin{itemize}
    \item Berechne $o = \Theta(w^T x)$
    \item Falls $o > 0$ und $t = 0$
    \begin{itemize}
        \item Setze $w = w - x$
    \end{itemize}
    \item Falls $o \leq 0$ und $t = 1$
    \begin{itemize}
        \item Setze $w = w + x$
    \end{itemize}
\end{itemize}
\section{Backpropagation}
Fehler Output Layer:
\begin{center}
    $-(t_d - o_d) \cdot \Theta'(W y) \cdot y_j$
\end{center}
\end{document}